#include "textflag.h"

// func encryptBlocks2Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks2Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X2
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	ADDQ $16, AX
	PXOR X2, X0
	PXOR X2, X1
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 16(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 16(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 16(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 32(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 48(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 64(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 80(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 96(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 112(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 128(AX), X2
	AESENC X2, X0
	AESENC X2, X1
	MOVUPS 144(AX), X2
	AESENCLAST X2, X0
	AESENCLAST X2, X1
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	RET

// func encryptBlocks4Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks4Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X4
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	ADDQ $16, AX
	PXOR X4, X0
	PXOR X4, X1
	PXOR X4, X2
	PXOR X4, X3
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 16(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 16(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 16(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 32(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 48(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 64(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 80(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 96(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 112(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 128(AX), X4
	AESENC X4, X0
	AESENC X4, X1
	AESENC X4, X2
	AESENC X4, X3
	MOVUPS 144(AX), X4
	AESENCLAST X4, X0
	AESENCLAST X4, X1
	AESENCLAST X4, X2
	AESENCLAST X4, X3
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	RET

// func encryptBlocks6Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks6Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X6
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	ADDQ $16, AX
	PXOR X6, X0
	PXOR X6, X1
	PXOR X6, X2
	PXOR X6, X3
	PXOR X6, X4
	PXOR X6, X5
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 16(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 16(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 16(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 32(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 48(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 64(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 80(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 96(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 112(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 128(AX), X6
	AESENC X6, X0
	AESENC X6, X1
	AESENC X6, X2
	AESENC X6, X3
	AESENC X6, X4
	AESENC X6, X5
	MOVUPS 144(AX), X6
	AESENCLAST X6, X0
	AESENCLAST X6, X1
	AESENCLAST X6, X2
	AESENCLAST X6, X3
	AESENCLAST X6, X4
	AESENCLAST X6, X5
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	RET

// func encryptBlocks8Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks8Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X8
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	MOVUPS 96(BX), X6
	MOVUPS 112(BX), X7
	ADDQ $16, AX
	PXOR X8, X0
	PXOR X8, X1
	PXOR X8, X2
	PXOR X8, X3
	PXOR X8, X4
	PXOR X8, X5
	PXOR X8, X6
	PXOR X8, X7
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 32(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 48(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 64(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 80(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 96(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 112(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 128(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 144(AX), X8
	AESENCLAST X8, X0
	AESENCLAST X8, X1
	AESENCLAST X8, X2
	AESENCLAST X8, X3
	AESENCLAST X8, X4
	AESENCLAST X8, X5
	AESENCLAST X8, X6
	AESENCLAST X8, X7
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	RET

// func encryptBlocks10Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks10Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X10
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	MOVUPS 96(BX), X6
	MOVUPS 112(BX), X7
	MOVUPS 128(BX), X8
	MOVUPS 144(BX), X9
	ADDQ $16, AX
	PXOR X10, X0
	PXOR X10, X1
	PXOR X10, X2
	PXOR X10, X3
	PXOR X10, X4
	PXOR X10, X5
	PXOR X10, X6
	PXOR X10, X7
	PXOR X10, X8
	PXOR X10, X9
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 16(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 16(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 16(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 32(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 48(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 64(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 80(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 96(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 112(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 128(AX), X10
	AESENC X10, X0
	AESENC X10, X1
	AESENC X10, X2
	AESENC X10, X3
	AESENC X10, X4
	AESENC X10, X5
	AESENC X10, X6
	AESENC X10, X7
	AESENC X10, X8
	AESENC X10, X9
	MOVUPS 144(AX), X10
	AESENCLAST X10, X0
	AESENCLAST X10, X1
	AESENCLAST X10, X2
	AESENCLAST X10, X3
	AESENCLAST X10, X4
	AESENCLAST X10, X5
	AESENCLAST X10, X6
	AESENCLAST X10, X7
	AESENCLAST X10, X8
	AESENCLAST X10, X9
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	MOVUPS X8, 128(DX)
	MOVUPS X9, 144(DX)
	RET

// func encryptBlocks12Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks12Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X12
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	MOVUPS 96(BX), X6
	MOVUPS 112(BX), X7
	MOVUPS 128(BX), X8
	MOVUPS 144(BX), X9
	MOVUPS 160(BX), X10
	MOVUPS 176(BX), X11
	ADDQ $16, AX
	PXOR X12, X0
	PXOR X12, X1
	PXOR X12, X2
	PXOR X12, X3
	PXOR X12, X4
	PXOR X12, X5
	PXOR X12, X6
	PXOR X12, X7
	PXOR X12, X8
	PXOR X12, X9
	PXOR X12, X10
	PXOR X12, X11
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 16(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 16(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 16(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 32(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 48(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 64(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 80(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 96(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 112(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 128(AX), X12
	AESENC X12, X0
	AESENC X12, X1
	AESENC X12, X2
	AESENC X12, X3
	AESENC X12, X4
	AESENC X12, X5
	AESENC X12, X6
	AESENC X12, X7
	AESENC X12, X8
	AESENC X12, X9
	AESENC X12, X10
	AESENC X12, X11
	MOVUPS 144(AX), X12
	AESENCLAST X12, X0
	AESENCLAST X12, X1
	AESENCLAST X12, X2
	AESENCLAST X12, X3
	AESENCLAST X12, X4
	AESENCLAST X12, X5
	AESENCLAST X12, X6
	AESENCLAST X12, X7
	AESENCLAST X12, X8
	AESENCLAST X12, X9
	AESENCLAST X12, X10
	AESENCLAST X12, X11
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	MOVUPS X8, 128(DX)
	MOVUPS X9, 144(DX)
	MOVUPS X10, 160(DX)
	MOVUPS X11, 176(DX)
	RET

// func encryptBlocks14Asm(nr int, xk *uint32, dst, src *byte)
TEXT ·encryptBlocks14Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X14
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	MOVUPS 96(BX), X6
	MOVUPS 112(BX), X7
	MOVUPS 128(BX), X8
	MOVUPS 144(BX), X9
	MOVUPS 160(BX), X10
	MOVUPS 176(BX), X11
	MOVUPS 192(BX), X12
	MOVUPS 208(BX), X13
	ADDQ $16, AX
	PXOR X14, X0
	PXOR X14, X1
	PXOR X14, X2
	PXOR X14, X3
	PXOR X14, X4
	PXOR X14, X5
	PXOR X14, X6
	PXOR X14, X7
	PXOR X14, X8
	PXOR X14, X9
	PXOR X14, X10
	PXOR X14, X11
	PXOR X14, X12
	PXOR X14, X13
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 16(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 16(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 16(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 32(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 48(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 64(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 80(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 96(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 112(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 128(AX), X14
	AESENC X14, X0
	AESENC X14, X1
	AESENC X14, X2
	AESENC X14, X3
	AESENC X14, X4
	AESENC X14, X5
	AESENC X14, X6
	AESENC X14, X7
	AESENC X14, X8
	AESENC X14, X9
	AESENC X14, X10
	AESENC X14, X11
	AESENC X14, X12
	AESENC X14, X13
	MOVUPS 144(AX), X14
	AESENCLAST X14, X0
	AESENCLAST X14, X1
	AESENCLAST X14, X2
	AESENCLAST X14, X3
	AESENCLAST X14, X4
	AESENCLAST X14, X5
	AESENCLAST X14, X6
	AESENCLAST X14, X7
	AESENCLAST X14, X8
	AESENCLAST X14, X9
	AESENCLAST X14, X10
	AESENCLAST X14, X11
	AESENCLAST X14, X12
	AESENCLAST X14, X13
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	MOVUPS X8, 128(DX)
	MOVUPS X9, 144(DX)
	MOVUPS X10, 160(DX)
	MOVUPS X11, 176(DX)
	MOVUPS X12, 192(DX)
	MOVUPS X13, 208(DX)
	RET

