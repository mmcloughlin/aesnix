#include "textflag.h"

// func encryptBlocks8Asm(nr int, xk *uint32, dst, src *byte)
TEXT Â·encryptBlocks8Asm(SB),NOSPLIT,$0
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ src+24(FP), BX
	MOVUPS 0(AX), X8
	MOVUPS 0(BX), X0
	MOVUPS 16(BX), X1
	MOVUPS 32(BX), X2
	MOVUPS 48(BX), X3
	MOVUPS 64(BX), X4
	MOVUPS 80(BX), X5
	MOVUPS 96(BX), X6
	MOVUPS 112(BX), X7
	ADDQ $16, AX
	PXOR X8, X0
	PXOR X8, X1
	PXOR X8, X2
	PXOR X8, X3
	PXOR X8, X4
	PXOR X8, X5
	PXOR X8, X6
	PXOR X8, X7
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 32(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 48(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 64(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 80(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 96(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 112(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 128(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 144(AX), X8
	AESENCLAST X8, X0
	AESENCLAST X8, X1
	AESENCLAST X8, X2
	AESENCLAST X8, X3
	AESENCLAST X8, X4
	AESENCLAST X8, X5
	AESENCLAST X8, X6
	AESENCLAST X8, X7
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	RET
